{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60259808",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7582b5a7",
   "metadata": {},
   "source": [
    "## Steps in Creating the AI ChatBOT\n",
    "\n",
    "1) Choosing a pre-fine tuned model\n",
    "2) Loading a pre trained model\n",
    "3) Deploying it with Wikipedia API\n",
    "4) Integrate it with Gradio UI to make it look interactive and prettier\n",
    "5) Test the bot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae91234",
   "metadata": {},
   "source": [
    "### Choosing a pre-fine tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc4ec81",
   "metadata": {},
   "source": [
    "I have choosen a model that is pre trained on the Stanford Question Answering Dataset (SQuAD) dataset as the training dataset from the hugging face repository. I chose BERT trained model for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f65a6bc",
   "metadata": {},
   "source": [
    "#### Loading a pre trained model\n",
    "> Link: https://huggingface.co/deepset/bert-base-cased-squad2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c764b497",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b99988190a462ea3334fb935e52945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install wikipedia\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1d9732",
   "metadata": {},
   "source": [
    "#### Playing with our model\n",
    "\n",
    "> * Tokenize the Input\n",
    "> * Obtain model scores\n",
    "> * Get the result\n",
    "\n",
    "Documentation Link: https://huggingface.co/docs/transformers/notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5ce61516",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Christopher Nolan'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = ''' Interstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher Nolan. \n",
    "            It stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, Matt Damon, and Michael \n",
    "            Caine. Set in a dystopian future where humanity is struggling to survive, the film follows a group of astronauts \n",
    "            who travel through a wormhole near Saturn in search of a new home for mankind. Interstellar premiered on October 26,\n",
    "            2014, in Los Angeles. In the United States, it was first released on film stock, expanding to venues using digital \n",
    "            projectors. The film had a worldwide gross over $677 million (and $773 million with subsequent re-releases), making \n",
    "            it the tenth-highest grossing film of 2014. It received acclaim for its performances, direction, screenplay, musical\n",
    "            score, visual effects, ambition, themes, and emotional weight. It has also received praise from many astronomers for\n",
    "            its scientific accuracy and portrayal of theoretical astrophysics. Since its premiere, Interstellar gained a cult \n",
    "            following, and now is regarded by many sci-fi experts as one of the best science-fiction films of all time.\n",
    "            Interstellar was nominated for five awards at the 87th Academy Awards, winning Best Visual Effects, and received \n",
    "            numerous other accolades. '''\n",
    "\n",
    "question = \"Who directed Interstellar\"\n",
    "\n",
    "# 1. Tokenize the input\n",
    "# Here return_tensors = \"pt\" returns a torch.Tensor objects.\n",
    "inputs = tokenizer.encode_plus(question, context, return_tensors = \"pt\") \n",
    "\n",
    "# 2. Obtain model scores\n",
    "answer_start_scores, answer_end_scores = model(**inputs, return_dict=False)\n",
    "# Get the most likely beginning of the answer\n",
    "answer_start = torch.argmax(answer_start_scores) \n",
    "# Get the most likely end of the answer\n",
    "answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "# 3. Get the result\n",
    "# Converting the start and end tokens back to words\n",
    "tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888ac7a2",
   "metadata": {},
   "source": [
    "#### Deploying it with Wikipedia API\n",
    "\n",
    "\n",
    "##### Searching keywords on Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "623a785c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Christopher Nolan\n"
     ]
    }
   ],
   "source": [
    "import wikipedia as wiki\n",
    "\n",
    "# Define the question\n",
    "question = \"Who directed Interstellar?\"\n",
    "\n",
    "# Use the Wikipedia API to search for the most relevant page based on the question\n",
    "search_results = wiki.search(question)\n",
    "if not search_results:\n",
    "    answer = \"Sorry, I could not find any relevant information on Wikipedia.\"\n",
    "else:\n",
    "    # Use the first search result as the page title\n",
    "    page_title = search_results[0]\n",
    "    # Get the Wikipedia page for the page title\n",
    "    page = wiki.page(page_title)\n",
    "    # Use the page content as the context for the Q&A model\n",
    "    context = page.content\n",
    "\n",
    "    # Set a maximum sequence length for the input\n",
    "    max_seq_length = 512\n",
    "\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer.encode_plus(question, context, max_length=max_seq_length, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
    "\n",
    "    # Obtain model scores\n",
    "    answer_start_scores, answer_end_scores = model(**inputs, return_dict=False)\n",
    "\n",
    "    # Get the most likely beginning of the answer\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    # Get the most likely end of the answer\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "    # Convert the start and end tokens back to words\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ee657790",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask a question ?Who directed Interstellar\n",
      "Christopher Nolan\n"
     ]
    }
   ],
   "source": [
    "# Define the question\n",
    "question = input(\"Ask a question ?\")\n",
    "\n",
    "# Use the Wikipedia API to search for the most relevant page based on the question\n",
    "search_results = wiki.search(question)\n",
    "if not search_results:\n",
    "    answer = \"Sorry, I could not find any relevant information on Wikipedia.\"\n",
    "else:\n",
    "    # Use the first search result as the page title\n",
    "    page_title = search_results[0]\n",
    "    # Get the Wikipedia page for the page title\n",
    "    page = wiki.page(page_title)\n",
    "    # Use the page content as the context for the Q&A model\n",
    "    context = page.content\n",
    "\n",
    "    # Set a maximum sequence length for the input\n",
    "    max_seq_length = 512\n",
    "\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer.encode_plus(question, context, max_length=max_seq_length, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
    "\n",
    "    # Obtain model scores\n",
    "    answer_start_scores, answer_end_scores = model(**inputs, return_dict=False)\n",
    "\n",
    "    # Get the most likely beginning of the answer\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    # Get the most likely end of the answer\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "    # Convert the start and end tokens back to words\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d89c9b6f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask a question ?Who was the founder of Mughal Empire in the India subcontinent?\n",
      "Babur\n"
     ]
    }
   ],
   "source": [
    "# Define the question\n",
    "question = input(\"Ask a question ?\")\n",
    "\n",
    "# Use the Wikipedia API to search for the most relevant page based on the question\n",
    "search_results = wiki.search(question)\n",
    "if not search_results:\n",
    "    answer = \"Sorry, I could not find any relevant information on Wikipedia.\"\n",
    "else:\n",
    "    # Use the first search result as the page title\n",
    "    page_title = search_results[0]\n",
    "    # Get the Wikipedia page for the page title\n",
    "    page = wiki.page(page_title)\n",
    "    # Use the page content as the context for the Q&A model\n",
    "    context = page.content\n",
    "\n",
    "    # Set a maximum sequence length for the input\n",
    "    max_seq_length = 512\n",
    "\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer.encode_plus(question, context, max_length=max_seq_length, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
    "\n",
    "    # Obtain model scores\n",
    "    answer_start_scores, answer_end_scores = model(**inputs, return_dict=False)\n",
    "\n",
    "    # Get the most likely beginning of the answer\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    # Get the most likely end of the answer\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "    # Convert the start and end tokens back to words\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0d4caa4c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask a question ?  Who is Elon Musk?\n",
      "[CLS]\n"
     ]
    }
   ],
   "source": [
    "# Define the question\n",
    "question = input(\"Ask a question ?  \")\n",
    "\n",
    "# Use the Wikipedia API to search for the most relevant page based on the question\n",
    "search_results = wiki.search(question)\n",
    "if not search_results:\n",
    "    answer = \"Sorry, I could not find any relevant information on Wikipedia.\"\n",
    "else:\n",
    "    # Use the first search result as the page title\n",
    "    page_title = search_results[0]\n",
    "    # Get the Wikipedia page for the page title\n",
    "    page = wiki.page(page_title)\n",
    "    # Use the page content as the context for the Q&A model\n",
    "    context = page.content\n",
    "\n",
    "    # Set a maximum sequence length for the input\n",
    "    max_seq_length = 512\n",
    "\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer.encode_plus(question, context, max_length=max_seq_length, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
    "\n",
    "    # Obtain model scores\n",
    "    answer_start_scores, answer_end_scores = model(**inputs, return_dict=False)\n",
    "\n",
    "    # Get the most likely beginning of the answer\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    # Get the most likely end of the answer\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "    # Convert the start and end tokens back to words\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2047d2bd",
   "metadata": {},
   "source": [
    "**Error** - The Expected output should be a summary of Elon musk But It returns a [CLS], that is because the bot fails to find an answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33abb291",
   "metadata": {},
   "source": [
    "### Putting it all togeather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41ce25c",
   "metadata": {},
   "source": [
    "#### Gradio UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "217bb5f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradioNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading gradio-3.20.1-py3-none-any.whl (14.3 MB)\n",
      "     ---------------------------------------- 0.0/14.3 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/14.3 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/14.3 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/14.3 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/14.3 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/14.3 MB 164.3 kB/s eta 0:01:27\n",
      "     --------------------------------------- 0.1/14.3 MB 306.3 kB/s eta 0:00:47\n",
      "     --------------------------------------- 0.1/14.3 MB 306.3 kB/s eta 0:00:47\n",
      "     --------------------------------------- 0.1/14.3 MB 306.3 kB/s eta 0:00:47\n",
      "     --------------------------------------- 0.1/14.3 MB 273.1 kB/s eta 0:00:52\n",
      "     --------------------------------------- 0.2/14.3 MB 364.8 kB/s eta 0:00:39\n",
      "     --------------------------------------- 0.2/14.3 MB 374.9 kB/s eta 0:00:38\n",
      "     --------------------------------------- 0.2/14.3 MB 374.9 kB/s eta 0:00:38\n",
      "      -------------------------------------- 0.2/14.3 MB 318.9 kB/s eta 0:00:45\n",
      "      -------------------------------------- 0.3/14.3 MB 454.0 kB/s eta 0:00:31\n",
      "      -------------------------------------- 0.3/14.3 MB 480.0 kB/s eta 0:00:30\n",
      "      -------------------------------------- 0.3/14.3 MB 480.0 kB/s eta 0:00:30\n",
      "      -------------------------------------- 0.3/14.3 MB 480.0 kB/s eta 0:00:30\n",
      "     - ------------------------------------- 0.4/14.3 MB 517.0 kB/s eta 0:00:27\n",
      "     - ------------------------------------- 0.6/14.3 MB 655.9 kB/s eta 0:00:21\n",
      "     - ------------------------------------- 0.6/14.3 MB 655.6 kB/s eta 0:00:21\n",
      "     - ------------------------------------- 0.6/14.3 MB 655.6 kB/s eta 0:00:21\n",
      "     - ------------------------------------- 0.7/14.3 MB 676.4 kB/s eta 0:00:21\n",
      "     -- ------------------------------------ 0.9/14.3 MB 864.1 kB/s eta 0:00:16\n",
      "     --- ------------------------------------ 1.1/14.3 MB 1.0 MB/s eta 0:00:14\n",
      "     --- ------------------------------------ 1.1/14.3 MB 1.0 MB/s eta 0:00:14\n",
      "     --- ------------------------------------ 1.1/14.3 MB 1.0 MB/s eta 0:00:14\n",
      "     ---- ----------------------------------- 1.5/14.3 MB 1.2 MB/s eta 0:00:11\n",
      "     ----- ---------------------------------- 2.1/14.3 MB 1.6 MB/s eta 0:00:08\n",
      "     ----- ---------------------------------- 2.1/14.3 MB 1.7 MB/s eta 0:00:08\n",
      "     ----- ---------------------------------- 2.1/14.3 MB 1.7 MB/s eta 0:00:08\n",
      "     ------ --------------------------------- 2.4/14.3 MB 1.7 MB/s eta 0:00:07\n",
      "     ------- -------------------------------- 2.8/14.3 MB 1.9 MB/s eta 0:00:06\n",
      "     -------- ------------------------------- 2.9/14.3 MB 1.9 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 3.3/14.3 MB 2.2 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 3.4/14.3 MB 2.1 MB/s eta 0:00:06\n",
      "     ---------- ----------------------------- 3.8/14.3 MB 2.3 MB/s eta 0:00:05\n",
      "     ----------- ---------------------------- 4.1/14.3 MB 2.4 MB/s eta 0:00:05\n",
      "     ------------ --------------------------- 4.4/14.3 MB 2.5 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 4.7/14.3 MB 2.6 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 5.0/14.3 MB 2.7 MB/s eta 0:00:04\n",
      "     --------------- ------------------------ 5.4/14.3 MB 2.9 MB/s eta 0:00:04\n",
      "     --------------- ------------------------ 5.7/14.3 MB 2.9 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 6.0/14.3 MB 3.0 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 6.3/14.3 MB 3.1 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 6.7/14.3 MB 3.2 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 7.0/14.3 MB 3.3 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 7.4/14.3 MB 3.4 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 7.7/14.3 MB 3.5 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 8.1/14.3 MB 3.6 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 8.5/14.3 MB 3.7 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 8.8/14.3 MB 3.7 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 9.2/14.3 MB 3.8 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 9.5/14.3 MB 3.9 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 9.9/14.3 MB 4.0 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 10.3/14.3 MB 4.3 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 10.7/14.3 MB 5.8 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 11.0/14.3 MB 6.4 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 11.4/14.3 MB 6.9 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 11.9/14.3 MB 6.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 12.3/14.3 MB 6.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 12.6/14.3 MB 7.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 13.1/14.3 MB 7.4 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 13.5/14.3 MB 7.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 13.9/14.3 MB 7.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  14.3/14.3 MB 7.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  14.3/14.3 MB 7.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 14.3/14.3 MB 7.4 MB/s eta 0:00:00\n",
      "Collecting orjson\n",
      "  Downloading orjson-3.8.7-cp310-none-win_amd64.whl (202 kB)\n",
      "     ---------------------------------------- 0.0/202.8 kB ? eta -:--:--\n",
      "     -------------------------------------- 202.8/202.8 kB 6.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyyaml in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from gradio) (6.0)\n",
      "Collecting fastapi\n",
      "  Downloading fastapi-0.93.0-py3-none-any.whl (56 kB)\n",
      "     ---------------------------------------- 0.0/56.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 56.3/56.3 kB ? eta 0:00:00\n",
      "Requirement already satisfied: pandas in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from gradio) (1.5.3)\n",
      "Collecting python-multipart\n",
      "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 0.0/45.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 45.7/45.7 kB 2.2 MB/s eta 0:00:00\n",
      "Collecting httpx\n",
      "  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n",
      "     ---------------------------------------- 0.0/71.5 kB ? eta -:--:--\n",
      "     ---------------------------------- ----- 61.4/71.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 71.5/71.5 kB 1.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from gradio) (4.4.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from gradio) (3.1.2)\n",
      "Collecting mdit-py-plugins<=0.3.3\n",
      "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
      "     ---------------------------------------- 0.0/50.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 50.5/50.5 kB 874.7 kB/s eta 0:00:00\n",
      "Collecting markdown-it-py[linkify]>=2.0.0\n",
      "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
      "     ---------------------------------------- 0.0/84.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 84.5/84.5 kB 4.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from gradio) (1.23.5)\n",
      "Collecting aiofiles\n",
      "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting ffmpy\n",
      "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting websockets>=10.0\n",
      "  Downloading websockets-10.4-cp310-cp310-win_amd64.whl (101 kB)\n",
      "     ---------------------------------------- 0.0/101.4 kB ? eta -:--:--\n",
      "     -------------------------------------- 101.4/101.4 kB 5.7 MB/s eta 0:00:00\n",
      "Collecting uvicorn\n",
      "  Downloading uvicorn-0.20.0-py3-none-any.whl (56 kB)\n",
      "     ---------------------------------------- 0.0/56.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/56.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/56.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/56.9 kB ? eta -:--:--\n",
      "     ------------------------------------ --- 51.2/56.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 56.9/56.9 kB 3.1 MB/s eta 0:00:00\n",
      "Collecting pycryptodome\n",
      "  Downloading pycryptodome-3.17-cp35-abi3-win_amd64.whl (1.7 MB)\n",
      "     ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.1/1.7 MB 4.3 MB/s eta 0:00:01\n",
      "     ------ --------------------------------- 0.3/1.7 MB 3.4 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 0.4/1.7 MB 3.1 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 0.5/1.7 MB 3.0 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 0.8/1.7 MB 3.4 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 1.1/1.7 MB 4.1 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 1.5/1.7 MB 4.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.7/1.7 MB 4.8 MB/s eta 0:00:00\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2023.3.0-py3-none-any.whl (145 kB)\n",
      "     ---------------------------------------- 0.0/145.4 kB ? eta -:--:--\n",
      "     -------------------------------------- 145.4/145.4 kB 9.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from gradio) (2.28.1)\n",
      "Requirement already satisfied: pydantic in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from gradio) (1.10.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from gradio) (9.4.0)\n",
      "Collecting altair>=4.2.0\n",
      "  Downloading altair-4.2.2-py3-none-any.whl (813 kB)\n",
      "     ---------------------------------------- 0.0/813.6 kB ? eta -:--:--\n",
      "     -------------- ----------------------- 317.4/813.6 kB 9.9 MB/s eta 0:00:01\n",
      "     -------------------------------- ----- 686.1/813.6 kB 8.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 813.6/813.6 kB 7.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: matplotlib in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from gradio) (3.7.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from gradio) (3.8.3)\n",
      "Collecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: markupsafe in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from gradio) (2.1.1)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from altair>=4.2.0->gradio) (0.4)\n",
      "Collecting toolz\n",
      "  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "     ---------------------------------------- 0.0/55.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 55.8/55.8 kB 2.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from altair>=4.2.0->gradio) (4.17.3)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting linkify-it-py<3,>=1\n",
      "  Downloading linkify_it_py-2.0.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from pandas->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from pandas->gradio) (2022.7)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from aiohttp->gradio) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from aiohttp->gradio) (1.8.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from aiohttp->gradio) (22.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from aiohttp->gradio) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from aiohttp->gradio) (6.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from aiohttp->gradio) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from aiohttp->gradio) (2.0.4)\n",
      "Collecting starlette<0.26.0,>=0.25.0\n",
      "  Downloading starlette-0.25.0-py3-none-any.whl (66 kB)\n",
      "     ---------------------------------------- 0.0/66.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 66.4/66.4 kB ? eta 0:00:00\n",
      "Requirement already satisfied: sniffio in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from httpx->gradio) (1.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from httpx->gradio) (2022.12.7)\n",
      "Collecting httpcore<0.17.0,>=0.15.0\n",
      "  Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\n",
      "     ---------------------------------------- 0.0/69.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 69.6/69.6 kB 3.7 MB/s eta 0:00:00\n",
      "Collecting rfc3986[idna2008]<2,>=1.3\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from matplotlib->gradio) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from matplotlib->gradio) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from matplotlib->gradio) (4.25.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from matplotlib->gradio) (1.0.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from matplotlib->gradio) (22.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from matplotlib->gradio) (0.11.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from requests->gradio) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from requests->gradio) (3.4)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from uvicorn->gradio) (8.0.4)\n",
      "Collecting h11>=0.8\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 58.3/58.3 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from click>=7.0->uvicorn->gradio) (0.4.6)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from httpcore<0.17.0,>=0.15.0->httpx->gradio) (3.5.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.18.0)\n",
      "Collecting uc-micro-py\n",
      "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\navinz\\desktop\\aichatbot\\env\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->gradio) (1.16.0)\n",
      "Building wheels for collected packages: ffmpy\n",
      "  Building wheel for ffmpy (setup.py): started\n",
      "  Building wheel for ffmpy (setup.py): finished with status 'done'\n",
      "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4712 sha256=070d3aa329767762c133e3d605136b1acfe53dcf5e6932b2605f5ebd6fcd32b7\n",
      "  Stored in directory: c:\\users\\navinz\\appdata\\local\\pip\\cache\\wheels\\0c\\c2\\0e\\3b9c6845c6a4e35beb90910cc70d9ac9ab5d47402bd62af0df\n",
      "Successfully built ffmpy\n",
      "Installing collected packages: rfc3986, pydub, ffmpy, websockets, uc-micro-py, toolz, python-multipart, pycryptodome, orjson, mdurl, h11, fsspec, aiofiles, uvicorn, starlette, markdown-it-py, linkify-it-py, httpcore, mdit-py-plugins, httpx, fastapi, altair, gradio\n",
      "Successfully installed aiofiles-23.1.0 altair-4.2.2 fastapi-0.93.0 ffmpy-0.3.0 fsspec-2023.3.0 gradio-3.20.1 h11-0.14.0 httpcore-0.16.3 httpx-0.23.3 linkify-it-py-2.0.0 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 mdurl-0.1.2 orjson-3.8.7 pycryptodome-3.17 pydub-0.25.1 python-multipart-0.0.6 rfc3986-1.5.0 starlette-0.25.0 toolz-0.12.0 uc-micro-py-1.0.1 uvicorn-0.20.0 websockets-10.4\n"
     ]
    }
   ],
   "source": [
    "#pip install gradio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "dead2734",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7877\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7877/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Load the pre-trained Q&A model and tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")\n",
    "\n",
    "# Define the function to get the answer\n",
    "def get_answer(question):\n",
    "    \n",
    "    # Use the Wikipedia API to search for the most relevant page based on the question\n",
    "    search_results = wiki.search(question)\n",
    "    if not search_results:\n",
    "        answer = \"Sorry, I could not find any relevant information on Wikipedia.\"\n",
    "    else:\n",
    "        \n",
    "        # Use the first search result as the page title\n",
    "        page_title = search_results[0]\n",
    "        \n",
    "        # Get the Wikipedia page for the page title\n",
    "        page = wiki.page(page_title)\n",
    "        \n",
    "        # Use the page content as the context for the Q&A model\n",
    "        context = page.content\n",
    "\n",
    "        # Set a maximum sequence length for the input\n",
    "        max_seq_length = 512\n",
    "\n",
    "        # Tokenize the input\n",
    "        inputs = tokenizer.encode_plus(question, context, max_length=max_seq_length, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
    "\n",
    "        # Obtain model scores\n",
    "        answer_start_scores, answer_end_scores = model(**inputs, return_dict=False)\n",
    "\n",
    "        # Get the most likely beginning of the answer\n",
    "        answer_start = torch.argmax(answer_start_scores)\n",
    "        # Get the most likely end of the answer\n",
    "        answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "        # Convert the start and end tokens back to words\n",
    "        answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Create the Gradio interface\n",
    "gr.Interface(\n",
    "    fn = get_answer,\n",
    "    inputs = gr.Textbox(label = \"Question\"),\n",
    "    outputs = \"text\",\n",
    "    title = \"Wikipedia Q&A ChatBOT\",\n",
    "    description = \"Type your question, I'm happy to get the answer from Wikipedia.\",\n",
    "    examples = [\n",
    "        [\"Who directed Interstellar?\"],\n",
    "        [\"Who is Robert James Moroso?\"],\n",
    "        [\"Who was the founder of Mughal Empire in the India subcontinent?\"],\n",
    "        [\"Where was Elijah Mudenda born?\"],\n",
    "        [\"What is Cassiopeia?\"],\n",
    "        [\"Which dynasty was established by Pushyavarman?\"],\n",
    "        [\"Featherstone Prison was constructed on property previously owned by?\"],\n",
    "        [\"Which team event did India won in the olympics?\"]\n",
    "    ]).launch();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa0c2e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
